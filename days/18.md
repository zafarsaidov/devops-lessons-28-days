[Main](../README.md)
---

# Service


## üéØ Objective

By the end of this practice, students will understand:
* What Kubernetes Services are and why we use them.
* How to expose Pods using ClusterIP and NodePort Services.
* How to access services internally and externally.

## üõ†Ô∏è Practice Setup

### ‚úÖ Step 1: Deploy a Sample Application

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web
        image: nginx
        ports:
        - containerPort: 80
```
Apply the file:
```bash
kubectl apply -f pod-deployment.yaml
kubectl get pods -l app=web
```

## üåê Part 1: ClusterIP Service

### ‚úÖ Step 2: Create a ClusterIP Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-clusterip
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
  type: ClusterIP
```
Apply and verify:
```bash
kubectl apply -f clusterip-service.yaml
kubectl get svc web-clusterip
```

### ‚úÖ Step 3: Access Service Internally (Using a Test Pod)

```bash
kubectl run curlpod --image=radial/busyboxplus:curl -it --restart=Never -- /bin/sh
# Inside pod:
curl web-clusterip
```

## üåç Part 2: NodePort Service

### ‚úÖ Step 4: Create a NodePort Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-nodeport
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
    protocol: TCP
  type: NodePort
```
Apply and verify:
```bash
kubectl apply -f nodeport-service.yaml
kubectl get svc web-nodeport
```

### ‚úÖ Step 5: Access NodePort From Browser or curl

Get node IP:

```bash
kubectl get nodes -o wide
```

Access in browser or curl:

```bash
curl http://<NODE-IP>:30080
```


### üßº Cleanup

```bash
kubectl delete -f pod-deployment.yaml
kubectl delete -f clusterip-service.yaml
kubectl delete -f nodeport-service.yaml
kubectl delete pod curlpod
```

## üß† Bonus Challenges
* Change the selector of a service and watch it stop working.
* Create two deployments (web and api) and expose both with different services.
* Create a ClusterIP service with port 8080 and targetPort 80.

# CoreDNS

## üéØ Learning Objectives

By the end of this session, students will understand:
* What CoreDNS is and its role in Kubernetes.
* How DNS-based service discovery works in Kubernetes.
* How to troubleshoot and test DNS resolution in a cluster.


## ‚öôÔ∏è Practice: CoreDNS Hands-On Guide

### ‚úÖ Step 1: Verify CoreDNS is Running

```bash
kubectl get pods -n kube-system -l k8s-app=kube-dns
```
Expected output:

```
NAME                       READY   STATUS    RESTARTS   AGE
coredns-xxx                1/1     Running   0          5d
```

### ‚úÖ Step 2: Inspect CoreDNS ConfigMap

```bash
kubectl -n kube-system get configmap coredns -o yaml
```

Example snippet:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
```

### ‚úÖ Step 3: Test DNS Resolution in the Cluster

#### Deploy a test Pod with DNS tools:

```bash
kubectl run dnsutils --image=busybox:1.28 --restart=Never -it --rm -- nslookup kubernetes
```

You should see a response resolving kubernetes.default.svc.cluster.local.
#### Try FQDN resolution:
```bash
nslookup nginx-service.default.svc.cluster.local
```
Replace nginx-service with your actual service.

### ‚úÖ Step 4: Create a Service and Test DNS Resolution

#### Create Deployment & Service:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: echo
  template:
    metadata:
      labels:
        app: echo
    spec:
      containers:
      - name: echo
        image: hashicorp/http-echo
        args:
        - "-text=hello"
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: echo
spec:
  selector:
    app: echo
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5678
```
Apply:
```bash
kubectl apply -f echo-deployment.yaml
```
#### Run a client pod and resolve the echo service:
```bash
kubectl run curlpod --image=radial/busyboxplus:curl -it --rm
# Inside pod:
nslookup echo
curl echo
```

### ‚úÖ Step 5: Troubleshooting DNS Issues

Check DNS logs:
```bash
kubectl logs -n kube-system -l k8s-app=kube-dns
```
Common issues:
* CoreDNS crashloop ‚Üí usually a syntax error in Corefile.
* No response ‚Üí network policies, iptables misconfig.
* Use dig or nslookup to test from multiple pods.

## üß† Bonus Tasks
* Modify CoreDNS ConfigMap to log DNS queries:
```json
.:53 {
  log
  errors
  ...
}
```
Then apply:
```bash
kubectl -n kube-system apply -f coredns-config.yaml
kubectl rollout restart deployment coredns -n kube-system
```
* Scale deployment to zero and check resolve services
* Execute pod and run command `cat /etc/resolv.conf`
* Check nameserver IP of pod and coreDNS service IP

## üßº Cleanup

```bash
kubectl delete deployment echo
kubectl delete svc echo
kubectl delete pod curlpod
```

## ‚úÖ Summary

| Concept | Example |
| ------ | ------|
| DNS server | CoreDNS |
| Service name FQDN | my-svc.my-namespace.svc.cluster.local |
| CoreDNS config | ConfigMap in kube-system |
| Debug tool | nslookup, dig, curl in a busybox pod |


[Debugging DNS Resolution](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)


# Ingress Controller & Rules


## üß± Ingress Structure
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 80
```


## ‚úÖ Step 1: Deploy NGINX Ingress Controller

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml
```
Wait for controller pod:
```bash
kubectl get pods -n ingress-nginx
```
Check external IP:
```bash
kubectl get svc -n ingress-nginx
```

## ‚úÖ Step 2: Deploy Sample Services

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web1
  template:
    metadata:
      labels:
        app: web1
    spec:
      containers:
      - name: web1
        image: hashicorp/http-echo
        args:
        - "-text=Hello from Web1"
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: web1
spec:
  selector:
    app: web1
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web2
  template:
    metadata:
      labels:
        app: web2
    spec:
      containers:
      - name: web2
        image: hashicorp/http-echo
        args:
        - "-text=Hello from Web2"
        ports:
        - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: web2
spec:
  selector:
    app: web2
  ports:
  - port: 80
    targetPort: 5678
```
Apply:
```bash
kubectl apply -f services.yaml
```

## ‚úÖ Step 3: Create an Ingress Resource

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /web1
        pathType: Prefix
        backend:
          service:
            name: web1
            port:
              number: 80
      - path: /web2
        pathType: Prefix
        backend:
          service:
            name: web2
            port:
              number: 80
```
Apply:
```bash
kubectl apply -f ingress.yaml
```

## ‚úÖ Step 4: Test Routing
Get the Ingress Controller external IP:
```bash
kubectl get svc ingress-nginx-controller -n ingress-nginx
```
Test from browser or curl:
```bash
curl http://<EXTERNAL-IP>/web1
curl http://<EXTERNAL-IP>/web2
```

## üß™ Bonus Exercises
* Add TLS with a self-signed certificate using Ingress.tls.
* Route based on host: instead of path: (e.g., web1.example.com).
* Test error handling when backend service is down.



## üßº Cleanup
```bash
kubectl delete -f ingress.yaml
kubectl delete -f services.yaml
kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml
```

[Documentation](https://kubernetes.io/docs/concepts/services-networking/ingress/)


# Network policy

## ‚úÖ Step 1: Setup Namespace and Pods
```bash
kubectl create namespace netpol-demo
```
## ‚úÖ Deploy 2 apps: frontend and backend

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: backend
  namespace: netpol-demo
  labels:
    app: backend
spec:
  containers:
  - name: backend
    image: hashicorp/http-echo
    args: ["-text=Hello from backend"]
    ports:
    - containerPort: 5678
---
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: netpol-demo
spec:
  selector:
    app: backend
  ports:
  - port: 80
    targetPort: 5678
---
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  namespace: netpol-demo
  labels:
    app: frontend
spec:
  containers:
  - name: curl
    image: curlimages/curl
    command: ["sleep", "3600"]
```
Apply:
```bash
kubectl apply -f netpol-demo.yaml
```

## ‚úÖ Step 2: Test Without Network Policy
```bash
kubectl exec -n netpol-demo frontend -- curl backend:80
# Expected output: Hello from backend
```

## ‚úÖ Step 3: Create a Deny-All Ingress Policy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: netpol-demo
spec:
  podSelector: {}  # Applies to all Pods
  policyTypes:
  - Ingress
```
Apply:
```bash
kubectl apply -f deny-all.yaml
```
Test again:
```bash
kubectl exec -n netpol-demo frontend -- curl backend:80
# Expected: connection refused or timeout
```

## ‚úÖ Step 4: Allow Ingress Only from Frontend

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend
  namespace: netpol-demo
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
  policyTypes:
  - Ingress
```
Apply:
```bash
kubectl apply -f allow-frontend.yaml
```
Test again:
```bash
kubectl exec -n netpol-demo frontend -- curl backend:80
# Expected: Hello from backend
```

## ‚úÖ Step 5: Deny All Egress from Frontend (Optional)

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-egress
  namespace: netpol-demo
spec:
  podSelector:
    matchLabels:
      app: frontend
  egress: []
  policyTypes:
  - Egress
```
Apply:
```bash
kubectl apply -f deny-egress.yaml
```
Try pinging external IP:
```bash
kubectl exec -n netpol-demo frontend -- curl http://1.1.1.1
# Should be blocked
```

## üß™ Bonus Tasks
* Allow egress only to specific IP (e.g., DNS or API).
* Use namespaceSelector to allow traffic only from Pods in a trusted namespace.
* Combine ingress and egress rules in one policy.

## üßº Cleanup
```bash
kubectl delete ns netpol-demo
```

## üß† Summary Table

| Feature	| Description |
| ------ | -------|
| podSelector |	Which pods the policy applies to |
| ingress |	Controls incoming connections |
| egress |	Controls outgoing traffic |
| policyTypes |	Ingress, Egress or both |
| Requires CNI |	Yes, e.g., Calico, Cilium, Weave |

[Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

[Main](../README.md)
---