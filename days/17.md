[Main](../README.md)
---

## Pod Phases
Pod phases represent the current status of a pod in its lifecycle. The primary pod phases include:

### Pending
A pod is in the â€œPendingâ€ phase when it has been accepted by the Kubernetes system but one or more of the containers have not been created yet. This could be due to scheduling issues or insufficient resources on the nodes.

__Scenario__: Deploy a pod that requests more resources than available on the node, leading to a â€œPendingâ€ phase.

### âœ… Summary of Key Kubernetes YAML Fields

| Field | Purpose | Example |
| --- | --- | --- |
| apiVersion | Specifies the API version | v1, apps/v1 |
| kind | Type of Kubernetes object | Pod, Deployment, Service |
| metadata | Metadata like name, labels | name: my-app |
| spec | Main configuration block | containers, replicas, etc. |
| template | Used in Deployment, Job, etc. | Pod template for replication |

---

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pending-pod
spec:
  containers:
  - name: busybox
    image: busybox
    resources:
      requests:
        memory: "10Gi" # Request more memory than available
        cpu: "4"
    command: ["sleep", "3600"]
```
Command to Apply:

```bash
kubectl apply -f pending-pod.yaml
```

__Explanation:__ This pod requests 10Gi of memory and 4 CPUs. If your nodes donâ€™t have these resources available, the pod will stay in the â€œPendingâ€ phase.

### Running

A pod is in the â€œRunningâ€ phase when all its containers are successfully created and at least one container is still running, or is in the process of starting or restarting.

__Scenario:__ Deploy a simple pod that starts successfully and enters the â€œRunningâ€ phase.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: running-pod
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
```
Command to Apply:
```bash
kubectl apply -f running-pod.yaml
```

__Explanation:__ This pod runs an Nginx server. Once deployed, it will likely transition to the â€œRunningâ€ phase almost immediately, assuming your cluster has enough resources.

### Succeeded

A pod enters the â€œSucceededâ€ phase when all its containers have terminated successfully with an exit code of 0, and they will not be restarted. This phase is common for pods with a finite task, such as batch jobs.

__Scenario:__ Deploy a pod with a simple job that completes successfully.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: succeeded-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["echo", "Hello, Kubernetes!"]
```
Command to Apply:
```bash
kubectl apply -f succeeded-pod.yaml
```
__Explanation:__ This pod runs a simple command and exits. After completing, it moves to the â€œSucceededâ€ phase.

### Failed

A pod is in the â€œFailedâ€ phase if any of its containers has terminated with a non-zero exit code or if a container failed to restart according to its restart policy.

__Scenario:__ Deploy a pod that runs a command designed to fail.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: failed-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["false"]  # This command will fail
```
Command to Apply:
```bash
kubectl apply -f failed-pod.yaml
```
__Explanation:__ This pod is configured to run a command that returns a non-zero exit code, causing the pod to enter the â€œFailedâ€ phase.


### Unknown

The â€œUnknownâ€ phase is when the state of the pod cannot be determined, typically due to communication issues between the node and the Kubernetes API.

### Pod Statuses: Completed vs. Error

In Kubernetes, pods can be in various statuses that reflect their current state. Two key statuses are â€œCompletedâ€ and â€œError.â€

#### Completed

__Meaning:__ The podâ€™s containers finished their work successfully and exited without issues.

__Use Case:__ This status is typical for jobs or tasks that are designed to run to completion, such as batch processing or data transformation tasks.

__Indicators__

__Status:__ Shows as â€œCompletedâ€ when all containers in the pod have exited with an exit code of 0.

__Example:__ A pod running a script that processes data and then exits cleanly will have the status â€œCompleted.â€

Command to Check:
```bash
kubectl get pod <pod-name>
```

#### Error
__Meaning:__ The podâ€™s containers encountered an issue and did not complete their tasks successfully. This status is used when a container in the pod exits with a non-zero exit code.

__Use Case:__ This status indicates a problem with the containerâ€™s execution, such as a configuration error or runtime issue.

__Indicators__

__Status:__ Shows as â€œErrorâ€ when containers have exited with a non-zero exit code and Kubernetes is unable to recover or restart them successfully.

__Example:__ A pod running a command that fails (e.g., a command with incorrect parameters) will have the status â€œError.â€

### Summary
â€œCompletedâ€ Status: Indicates successful execution of the podâ€™s tasks, with all containers exiting cleanly.

â€œErrorâ€ Status: Indicates that there were issues with the podâ€™s execution, resulting in one or more containers exiting with an error code.

- [ReplicaSets](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
- Deploments
    - [Documentation](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
    - [Blue-Green Deployments](https://medium.com/cloud-native-daily/blue-green-deployments-with-kubernetes-a-comprehensive-guide-5d196dad1976)
    - [Canary Deployment](https://medium.com/@muppedaanvesh/implementing-canary-deployment-in-kubernetes-0be4bc1e1aca)

# Labels and selectors

[Documentation](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

ğŸ¯ Objective

__Understand:__
* How to assign labels to Kubernetes resources.
* How to filter resources using label selectors.
* How Services, Deployments, and kubectl use label selectors.

## Practice Guide: Pod Labels & Selectors

### âœ… Step 1: Create Pods with Labels

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: web
    tier: frontend
spec:
  containers:
  - name: nginx
    image: nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: backend
  labels:
    app: web
    tier: backend
spec:
  containers:
  - name: httpd
    image: httpd
```

Apply the file:
```bash
kubectl apply -f pod-labels.yaml
```


### âœ… Step 2: View Labels on Pods

```bash
kubectl get pods --show-labels
```

Expected output shows label columns.

Or:
```bash
kubectl get pod frontend -o jsonpath="{.metadata.labels}"
```

### âœ… Step 3: Filter Pods Using Label Selectors

```bash
kubectl get pods -l app=web
kubectl get pods -l tier=frontend
kubectl get pods -l 'app=web,tier=frontend'
```

### âœ… Step 4: Label an Existing Pod (Add/Modify)

```bash
kubectl label pod backend env=prod
kubectl get pod backend --show-labels
```

Overwrite a label:

```bash
kubectl label pod backend env=dev --overwrite
```
Remove a label:
```bash
kubectl label pod backend env-
```


### âœ… Step 5: Use Label Selectors in a Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  selector:
    app: web
    tier: frontend
  ports:
  - port: 80
    targetPort: 80
```
Apply:
```bash
kubectl apply -f service-selector.yaml
kubectl get endpoints frontend-service
```

__Explanation:__ This service routes traffic only to the pod with both labels app=web and tier=frontend.


### âœ… Step 6: Advanced Selector Examples (MatchExpressions)
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug
  labels:
    role: test
    env: dev
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["sleep", "3600"]

# deployment-match-expressions.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-select
spec:
  replicas: 1
  selector:
    matchExpressions:
    - key: env
      operator: In
      values:
      - dev
      - test
  template:
    metadata:
      labels:
        app: demo
        env: dev
    spec:
      containers:
      - name: nginx
        image: nginx

```

### ğŸ§¼ Cleanup

```bash
kubectl delete -f pod-labels.yaml
kubectl delete -f service-selector.yaml
kubectl delete pod debug
kubectl delete deployment demo-select
```

## Imperative way

[Imperative Commands](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/imperative-command/)


# Kubernetes Volumes

## emptyDir Volume

ğŸ“ Task

Create a pod that uses an emptyDir volume shared between two containers.

âœ… Example YAML

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-demo
spec:
  containers:
  - name: writer
    image: busybox
    command: ['sh', '-c', 'echo "hello from writer" > /data/file && sleep 3600']
    volumeMounts:
    - mountPath: /data
      name: shared-data
  - name: reader
    image: busybox
    command: ['sh', '-c', 'sleep 5 && cat /data/file && sleep 3600']
    volumeMounts:
    - mountPath: /data
      name: shared-data
  volumes:
  - name: shared-data
    emptyDir: {}
```

## hostPath Volume

âœ… Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-demo
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo Hello > /host/log.txt && sleep 3600']
    volumeMounts:
    - mountPath: /host
      name: host-volume
  volumes:
  - name: host-volume
    hostPath:
      path: /tmp/data
      type: DirectoryOrCreate
```

## Create Persistent Volume (PV)

Use local node storage manually

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: local-pv
spec:
  capacity:
    storage: 500Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data
  persistentVolumeReclaimPolicy: Retain
```

## Create Persistent Volume Claim (PVC)

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Mi
```

## Create Pod Deployment with PVC

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvc-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pvc-demo
  template:
    metadata:
      labels:
        app: pvc-demo
    spec:
      containers:
      - name: app
        image: busybox
        command: ['sh', '-c', 'echo Hello from PVC > /data/hello.txt && sleep 3600']
        volumeMounts:
        - mountPath: /data
          name: vol
      volumes:
      - name: vol
        persistentVolumeClaim:
          claimName: local-pvc
```

## StorageClass + Dynamic PVC (with Local Provisioner or External)

ğŸ¯ StorageClass Example (Local Volume Provisioner)

__Prerequisite:__ Configure local volume provisioner or use hostPath in kind/minikube.

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

### ğŸ“ Dynamic PVC Example
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 100Mi
```

### ğŸ§ª Pod using dynamic PVC
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dynamic-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'echo using dynamic PVC > /mnt/msg && sleep 3600']
    volumeMounts:
    - mountPath: /mnt
      name: vol
  volumes:
  - name: vol
    persistentVolumeClaim:
      claimName: dynamic-pvc
```

## Clean Up

```bash
kubectl delete pod,deploy,pvc,pv,sc --all
```

## ğŸ BONUS TASKS
* Retention Check:
    * Set PV reclaimPolicy to Retain
    * Delete the PVC â†’ check if volume is still there.
* Data Persistence:
    * Re-deploy the pod and verify data is retained.
* Multi-pod Access:
    * Deploy 2 pods sharing a ReadWriteMany (if available) volume.
* Volume Backup:
    * kubectl cp files from PVC-mounted path.
* ReadOnlyMount:
    * Mount a volume with readOnly: true and try to write.


# StatefulSets

## ğŸ¯ Objectives
* Understand the purpose and behavior of StatefulSets
* Use persistent volumes and a local StorageClass
* Observe predictable pod names, DNS, and scaling order
* Ensure data persistence across pod restarts

## Create a Local StorageClass

```yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```
Apply:
```bash
kubectl apply -f storageclass.yaml
```

## Create a Headless Service

Headless service is required for stable network identity of StatefulSet pods.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web
spec:
  clusterIP: None  # Important for headless service
  selector:
    app: nginx
  ports:
  - port: 80
    name: web
```

## Create a StatefulSet

This example deploys NGINX with 3 replicas, each with its own persistent volume.

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "web"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "local-storage"
      resources:
        requests:
          storage: 1Gi
```
Apply:
```bash
kubectl apply -f statefulset.yaml
```

## Examine Pod Creation Order

```bash
kubectl get pods -l app=nginx --watch
```
You will see pods named web-0, web-1, web-2 created in order.

## Check Pod DNS Names

```bash
kubectl exec web-0 -- nslookup web-0.web
kubectl exec web-1 -- nslookup web-1.web
```
Each pod is reachable via:
```
<statefulset-name>-<ordinal>.<service-name>
Example: web-0.web, web-1.web
```

## Verify Volume Data Integrity

Create a test file in web-0:
```bash
kubectl exec web-0 -- /bin/sh -c "echo web-0 > /usr/share/nginx/html/index.html"
```
Delete and recreate pod:
```bash
kubectl delete pod web-0
```
Check if data is still present after pod recreation:
```bash
kubectl exec web-0 -- cat /usr/share/nginx/html/index.html
```
âœ… You should see â€œweb-0â€, confirming volume persistence.

## Scale StatefulSet

```bash
kubectl scale statefulset web --replicas=5
kubectl get pods -l app=nginx
```
Then scale down:
```bash
kubectl scale statefulset web --replicas=2
kubectl get pods
```
__Observe:__

* Pods are terminated in reverse order (e.g., web-4, web-3 first)
* Volumes (PVCs) for terminated pods are retained unless manually deleted

## Cleanup
```bash
kubectl delete statefulset web
kubectl delete svc web
kubectl delete pvc -l app=nginx
kubectl delete sc local-storage
```

# DaemonSets

## ğŸ¯ Learning Objectives
* Understand what a DaemonSet is and its use cases
* Deploy a DaemonSet to run an agent on every node
* Verify that it runs one pod per node
* Explore real-world DaemonSets (e.g., log collectors, monitoring agents)

## ğŸ§  Theory: What Is a DaemonSet?

A DaemonSet ensures that a copy of a pod runs on all (or some) nodes in a cluster.

ğŸ›  Common Use Cases
* Log collection (e.g., Fluentd, Filebeat)
* Node monitoring (e.g., Prometheus Node Exporter)
* System-level configuration agents
* Networking plugins (CNI agents)

When a new node joins the cluster, the DaemonSet automatically schedules a pod on it.

## âš™ï¸ Syntax Example
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-logger
  labels:
    app: node-logger
spec:
  selector:
    matchLabels:
      name: node-logger
  template:
    metadata:
      labels:
        name: node-logger
    spec:
      containers:
      - name: logger
        image: busybox
        command: [ "/bin/sh", "-c", "while true; do echo Hello from $(hostname) >> /var/log/node.log; sleep 10; done" ]
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

## ğŸ§ª Practice Tasks

### Create the DaemonSet

Save the above manifest as daemonset.yaml, then:
```bash
kubectl apply -f daemonset.yaml
```
### Confirm Pod Scheduling
```bash
kubectl get pods -o wide -l name=node-logger
```
âœ… You should see one pod per node, each on a different node.

### Check Log File Written on Host

To verify that logs are written to the nodeâ€™s /var/log/node.log:

```bash
kubectl exec -it <daemon-pod-name> -- tail /var/log/node.log
```
This uses a hostPath mount â€” files written here are visible on the host.

### ğŸ” Real-World DaemonSet Example

Example: Install Prometheus Node Exporter as a DaemonSet

```yaml
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/manifests/setup/prometheus-node-exporter-daemonset.yaml
```

## ğŸ”„ Cleanup
```bash
kubectl delete daemonset node-logger
```


[Main](../README.md)
---