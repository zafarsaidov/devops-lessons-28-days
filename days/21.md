[Main](../README.md)
---

# üß™ Practice Lesson: Taints and Tolerations

üéØ Objectives:
* Understand how taints prevent pod scheduling
* Learn how tolerations allow pods to bypass taints
* Practice real-world use cases (dedicated node, soft restrictions, NoExecute evictions)

üîç Quick Refresher
* Taint: Applied to a node. Prevents pods from being scheduled unless they ‚Äútolerate‚Äù it.
* Toleration: Applied to a pod. Allows it to tolerate taints and be scheduled to tainted nodes.

## 1Ô∏è‚É£ Task: Taint a Node and Try to Schedule a Pod

‚úÖ Steps:

Get nodes
```bash
kubectl get nodes
```
Pick a node (e.g., worker2)
```bash
kubectl taint nodes worker2 key1=value1:NoSchedule
```
Now create a test pod without toleration:

no-toleration-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-toleration
spec:
  containers:
    - name: nginx
      image: nginx
```
```bash
kubectl apply -f no-toleration-pod.yaml
kubectl describe pod no-toleration
```
## üîç Check:
* Pod will remain in Pending state.
* Describe will show ‚Äútaint violation‚Äù.

## 2Ô∏è‚É£ Task: Add Toleration and Reapply

toleration-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: toleration-pod
spec:
  containers:
    - name: nginx
      image: nginx
  tolerations:
    - key: "key1"
      operator: "Equal"
      value: "value1"
      effect: "NoSchedule"
```
```bash
kubectl apply -f toleration-pod.yaml
kubectl get pod toleration-pod -o wide
```
## üîç Check:
* Pod should now be scheduled to worker2.

## 3Ô∏è‚É£ Task: Dedicated Node for Critical Workloads

Label a node:
```bash
kubectl label node worker2 dedicated=monitoring
kubectl taint node worker2 dedicated=monitoring:NoSchedule
```

Create a monitoring pod with toleration and node selector:

monitoring-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: monitoring
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
  tolerations:
    - key: "dedicated"
      operator: "Equal"
      value: "monitoring"
      effect: "NoSchedule"
  nodeSelector:
    dedicated: "monitoring"
```
```bash
kubectl apply -f monitoring-pod.yaml
```

‚úÖ Now try scheduling a pod without toleration ‚Äî it should not go to the tainted node.

## 4Ô∏è‚É£ Task: NoExecute Taint with Eviction

Apply NoExecute taint:
```bash
kubectl taint nodes worker2 test=evictme:NoExecute
```
Run a pod with no toleration on that node using nodeName:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: will-be-evicted
spec:
  nodeName: worker2
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
```
```bash
kubectl apply -f will-be-evicted.yaml
```

‚úÖ Pod will start, then be evicted after a few seconds.

Now try tolerating it:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: tolerated
spec:
  nodeName: worker2
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
  tolerations:
    - key: "test"
      operator: "Equal"
      value: "evictme"
      effect: "NoExecute"
      tolerationSeconds: 30
```
‚úÖ Pod stays for 30 seconds before eviction.

## 5Ô∏è‚É£ Bonus Task: Multiple Taints and Combined Tolerations

Taint node with multiple keys:
```bash
kubectl taint nodes worker2 role=backend:NoSchedule
kubectl taint nodes worker2 disk=ssd:NoSchedule
```
Create pod with partial toleration:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: partial
spec:
  containers:
    - name: nginx
      image: nginx
  tolerations:
    - key: "role"
      operator: "Equal"
      value: "backend"
      effect: "NoSchedule"
```
‚úÖ Pod will still not schedule. Now add second toleration to succeed.

## üîç Commands for Verifying

| Command |	Purpose |
| ---- | ---- |
| `kubectl get pods -o wide` |	Check pod scheduling |
| `kubectl describe pod <pod>` |	See taint/toleration mismatch |
| `kubectl describe node <node>` |	View taints on node |
| `kubectl get nodes --show-labels` |	View node labels |
| `kubectl taint nodes <node> <key>=<value>:<effect>` |	Add taint |
| `kubectl taint nodes <node> <key>-` |	Remove taint |


## üßπ Cleanup
```bash
kubectl delete pod --all
kubectl taint nodes <node> key1-
kubectl taint nodes <node> dedicated-
kubectl taint nodes <node> test-
kubectl taint nodes <node> role-
kubectl taint nodes <node> disk-
kubectl label node <node> dedicated-
```

# üß™ Practice Lesson: Resource Requests, Limits, ResourceQuota, and LimitRange

## üéØ Objectives
* Understand how resource requests and limits work
* Deploy pods with/without resource definitions
* Apply and test namespace ResourceQuota and LimitRange

## üìö Part 1: Resource Requests and Limits

### üß™ Task 1: Deploy Pods with Resource Requests and Limits

Create a pod with defined resource requests and limits:

pod-resources.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: stress-pod
spec:
  containers:
    - name: stress
      image: polinux/stress
      args: ["--cpu", "1"]
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "200m"
          memory: "256Mi"
```
```bash
kubectl apply -f pod-resources.yaml
kubectl describe pod stress-pod
```
‚úÖ Check: Pod will be scheduled only on nodes with at least 100m CPU and 128Mi memory available.

### üß™ Task 2: Observe Behavior Without Limits

pod-no-limits.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: no-limits
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "while true; do echo RUNNING; sleep 5; done"]
```
```bash
kubectl apply -f pod-no-limits.yaml
```
‚úÖ Observe: No constraints ‚Äî could use a lot of resources if under pressure.

## üì¶ Part 2: LimitRange

### üß™ Task 3: Create a Namespace and LimitRange
```bash
kubectl create ns dev-team
```
limitrange.yaml
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limitrange
  namespace: dev-team
spec:
  limits:
    - default:
        cpu: 500m
        memory: 256Mi
      defaultRequest:
        cpu: 100m
        memory: 128Mi
      type: Container
```
```bash
kubectl apply -f limitrange.yaml
```
### üß™ Task 4: Deploy Pod Without Resources to See Auto-applied Limits

pod-auto-resources.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: auto-resources
  namespace: dev-team
spec:
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
```
```bash
kubectl apply -f pod-auto-resources.yaml
kubectl describe pod auto-resources -n dev-team
```
‚úÖ Check: Resource requests/limits auto-assigned by LimitRange.

## üìä Part 3: ResourceQuota

### üß™ Task 5: Apply ResourceQuota in the Namespace

resourcequota.yaml
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev-team
spec:
  hard:
    pods: "5"
    requests.cpu: "500m"
    requests.memory: "512Mi"
    limits.cpu: "1"
    limits.memory: "1Gi"
```
```bash
kubectl apply -f resourcequota.yaml
```
### üß™ Task 6: Try to Deploy Pods to Exceed Quota

Create this pod and attempt to run it multiple times:

big-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: big-pod
  namespace: dev-team
spec:
  containers:
    - name: stress
      image: polinux/stress
      args: ["--cpu", "1"]
      resources:
        requests:
          cpu: "300m"
          memory: "300Mi"
        limits:
          cpu: "500m"
          memory: "600Mi"
```
```bash
kubectl apply -f big-pod.yaml -n dev-team
```
Try creating 2-3 copies and see rejections after quota exceeds

## ‚úÖ Check:
* Pods beyond quota are rejected with a FailedScheduling event.
* Use kubectl describe quota dev-quota -n dev-team to see usage.

## üîç Commands for Verification

| Command |	Purpose |
| ---- | ---- |
| `kubectl top pod` |	See live resource usage (metrics-server required) |
| `kubectl describe pod <name>` |	See assigned requests/limits |
| `kubectl describe limitrange -n <ns>` |	Inspect default resource policy |
| `kubectl describe quota -n <ns>` |	View quota usage and limits |
| `kubectl get pod -n <ns>` |	Check pod status |
| `kubectl logs <pod>` |	View pod output |


## üßπ Cleanup
```bash
kubectl delete ns dev-team
kubectl delete pod stress-pod no-limits
```

## üéÅ Bonus Task
* Try a Deployment instead of a Pod using resource limits.
* Create a LimitRange that restricts max CPU > 1 and see pod rejections.
* Set a quota for configmaps or secrets to limit excessive object creation.


# üß™ Practice Lesson: Pod Priority and Preemption


üéØ Learning Objectives

By the end of this lesson, students will be able to:
* Define and use PriorityClass
* Observe pod preemption behavior when cluster resources are limited
* Test eviction scenarios by applying priorities



üß± Theory in Practice (Quick Summary)
* PriorityClass defines the importance of a pod.
* When resources are limited, higher priority pods can preempt (evict) lower priority pods.
* Preemption only happens if there is no other way to schedule the pod.

## 1Ô∏è‚É£ Create Three Priority Classes

priorities.yaml
```yaml
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000
globalDefault: false
description: "For critical workloads"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 50000
globalDefault: false
description: "For regular workloads"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
globalDefault: false
description: "For background jobs"
...
```
```bash
kubectl apply -f high-priority.yaml
kubectl apply -f medium-priority.yaml
kubectl apply -f low-priority.yaml
kubectl get priorityclass
```

## 2Ô∏è‚É£ Fill Up the Cluster with Low Priority Pods

Create 5 pods using a memory-heavy image.

low-load.yaml:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: low-load-1
spec:
  priorityClassName: low-priority
  containers:
    - name: stress
      image: polinux/stress
      args: ["--vm", "1", "--vm-bytes", "200Mi", "--vm-hang", "1"]
      resources:
        requests:
          memory: "200Mi"
          cpu: "100m"
        limits:
          memory: "200Mi"
          cpu: "200m"
```
```bash
for i in {1..5}; do
  kubectl apply -f low-load.yaml -n default --dry-run=client -o yaml | \
  sed "s/low-load-1/low-load-$i/g" | kubectl apply -f -
done
```

‚úÖ Check: Pods should be scheduled and consume cluster memory.
```bash
kubectl top pods
kubectl get pods -o wide
```

## 3Ô∏è‚É£ Attempt to Schedule High Priority Pod

high-priority-pod.yaml:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high-priority-pod
spec:
  priorityClassName: high-priority
  containers:
    - name: nginx
      image: nginx
      resources:
        requests:
          memory: "300Mi"
          cpu: "100m"
```
```bash
kubectl apply -f high-priority-pod.yaml
```

‚úÖ Expected Result: Some low-priority pods will be evicted to free space for the high-priority pod.
```bash
kubectl get pods
kubectl describe pod high-priority-pod
kubectl get events --sort-by=.metadata.creationTimestamp
```

## 4Ô∏è‚É£ Try a Medium Priority Pod (No Preemption)

medium-priority-pod.yaml
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: medium-priority-pod
spec:
  priorityClassName: medium-priority
  containers:
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
      resources:
        requests:
          memory: "400Mi"
          cpu: "100m"
```
```bash
kubectl apply -f medium-priority-pod.yaml
```
‚úÖ If there‚Äôs no space, it will remain Pending, and no preemption will happen, because preemption only occurs if the incoming pod has higher priority.

## üîç Useful Verification Commands

| Command | Purpose |
| ---- | ---- |
| `kubectl get priorityclass` |	See all priorities |
| `kubectl top pods` |	Monitor resource usage |
| `kubectl describe pod <name>` |	Check preemption and events |
| `kubectl get events --sort-by=.metadata.creationTimestamp` |	See scheduling/preemption messages |
| `kubectl delete pod <name>` |	Free up space manually |


## üéÅ Bonus Tasks
* Create a Deployment with high-priority pods and scale it up ‚Äî see which pods are evicted.
* Set preemptionPolicy: Never in the high-priority pod to prevent eviction of others.
```yaml
preemptionPolicy: Never
```
* Observe that pods remain Pending.

## üßπ Cleanup
```bash
kubectl delete pod high-priority-pod medium-priority-pod
kubectl delete pod -l name=low-load
kubectl delete priorityclass high-priority medium-priority low-priority
```


# üß™ Practice Lesson: CronJobs in Kubernetes


## üéØ Objectives

By the end of this lesson, students will:
* Create and schedule CronJobs
* Understand retry behavior and concurrency policies
* Configure history retention and cleanup
* Monitor and debug job executions


## üß± Concepts Refresher

| Term | Meaning |
| ---- | ---- |
| CronJob |	A Kubernetes object that runs Jobs on a time-based schedule |
| Job |	Executes a pod to completion |
| Concurrency Policy |	Controls overlapping jobs: Allow, Forbid, Replace |
| StartingDeadlineSeconds |	Prevents missed schedules from starting after a delay |
| Successful/Failed History Limits |	Controls how many job pods to keep |

## üß™ Practice Tasks

### 1Ô∏è‚É£ Task: Create a Basic CronJob (every minute)

üìÑ cronjob-basic.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["sh", "-c", "date; echo Hello from the cluster"]
          restartPolicy: OnFailure
```
```bash
kubectl apply -f cronjob-basic.yaml
```
‚úÖ Verify:
```bash
kubectl get cronjob
kubectl get jobs --watch
kubectl logs job/<job-name>   # replace with real job name
```

### 2Ô∏è‚É£ Task: Observe Retries on Failure

üìÑ cronjob-failure.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: failme
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 3
      template:
        spec:
          containers:
          - name: fail
            image: busybox
            command: ["sh", "-c", "exit 1"]
          restartPolicy: OnFailure
```
```bash
kubectl apply -f cronjob-failure.yaml
```
‚úÖ Check retries:
```bash
kubectl describe job -l job-name=failme
kubectl get pods --selector=job-name=<name> -o wide
```

### 3Ô∏è‚É£ Task: Test Concurrency Policies

‚úÖ Allow (default): All jobs run even if previous not finished

üß™ Forbid: New job is skipped if previous is still running

üìÑ cronjob-forbid.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sleep-job
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Forbid
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: sleeper
            image: busybox
            command: ["sleep", "90"]
          restartPolicy: OnFailure
```
```bash
kubectl apply -f cronjob-forbid.yaml
```
‚úÖ Observe:
* Job will skip if previous one not done
* Use kubectl get jobs to see fewer jobs

### üß™ Replace: Replace running job with new one

üìÑ cronjob-replace.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: replace-job
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Replace
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: replacer
            image: busybox
            command: ["sleep", "90"]
          restartPolicy: OnFailure
```
‚úÖ Watch jobs:
```bash
kubectl get jobs --watch
```
* Only one job should remain active; old one gets terminated.

### 4Ô∏è‚É£ Task: Limit Job History (Cleanup)

üìÑ cronjob-cleanup.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job
spec:
  schedule: "*/1 * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleaner
            image: busybox
            command: ["date"]
          restartPolicy: OnFailure
```
‚úÖ Observe:
```bash
kubectl get jobs --watch
```
Only 1 successful and 1 failed job retained


### 5Ô∏è‚É£ Task: Prevent Missed Jobs with startingDeadlineSeconds

üìÑ cronjob-strict.yaml:
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: strict-start
spec:
  schedule: "*/1 * * * *"
  startingDeadlineSeconds: 10
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["sleep", "70"]
          restartPolicy: OnFailure
```
‚úÖ Simulate pause:
```bash
kubectl scale deployment/kube-controller-manager --replicas=0 -n kube-system
# wait 2 minutes
kubectl scale deployment/kube-controller-manager --replicas=1 -n kube-system
```
Observe: Missed schedule won‚Äôt run due to deadline.

### üîç Verification Commands

| Command | Purpose |
| ---- | ---- |
| `kubectl get cronjobs` |	View CronJobs |
| `kubectl get jobs` |	View job executions |
| `kubectl get pods --selector=job-name=<name>` |	See pods per job |
| `kubectl logs job/<job>` | 	Inspect job logs |
| `kubectl describe cronjob <name>` |	Check concurrency, deadline, history config |


### üßπ Cleanup
```bash
kubectl delete cronjob hello failme sleep-job replace-job cleanup-job strict-start
kubectl delete job --all
```

### üéÅ Bonus Challenges
* Deploy a CronJob that posts logs to a webhook
* Create a CronJob that backs up a volume to another pod using initContainers
* Combine CronJob with ResourceQuota and LimitRange in a namespace




[Main](../README.md)
---